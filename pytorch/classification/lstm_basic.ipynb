{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "from zh_nlp_demo.utils.tokenization import FullTokenizer\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# https://bravey.github.io/2020-05-12-使用LSTM+Pytorch对电影评论进行情感分类.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量：  9600\n",
      "验证集样本数量：  1200\n",
      "测试集样本数量：  1200\n",
      "训练集样本示例：\n",
      "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "[train_examples, dev_examples, test_examples] = load_dataset('chnsenticorp', splits=('train', 'dev', 'test'))\n",
    "\n",
    "print('训练集样本数量： ', len(train_examples))\n",
    "print('验证集样本数量： ', len(dev_examples))\n",
    "print('测试集样本数量： ', len(test_examples))\n",
    "\n",
    "print('训练集样本示例：')\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer('../../data/dict/vocab.txt')\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.data, self.label = self.examples_to_ids(tokenizer, examples)\n",
    "\n",
    "    def examples_to_ids(self, tokenizer, examples, maxlen=200, num_classes=2):\n",
    "        tokens = [tokenizer.tokenize(example['text']) for example in examples]\n",
    "        input_ids = [torch.tensor(tokenizer.convert_tokens_to_ids(tokens), dtype=torch.int64) for tokens in tokens]\n",
    "        input_y = [example['label'] for example in examples]\n",
    "        return input_ids, input_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    data_length = [len(sq[0]) for sq in batch]\n",
    "    input_data, input_label = [], []\n",
    "    for text, label in batch:\n",
    "        input_data.append(text)\n",
    "        input_label.append(label)\n",
    "    input_data = pad_sequence(input_data, batch_first=True, padding_value=0)\n",
    "    input_label = torch.tensor(input_label)\n",
    "    return input_data, input_label, data_length\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(tokenizer, train_examples)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 隐层包含向前层和向后层两层，所以隐层共有两倍的Hidden_size\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "\n",
    "        text_emb = self.embedding(x)\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[:, -1, :]\n",
    "        # out_reverse = output[:, 0, :]\n",
    "        # out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\n",
    "        text_fea = self.fc(out_forward)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "        return text_out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# 实例化一个Birectional RNN模型\n",
    "model = BiRNN(vocab_size, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/2400], Loss: 0.6917\n",
      "Epoch [1/2], Step [200/2400], Loss: 0.6801\n",
      "Epoch [1/2], Step [300/2400], Loss: 0.6923\n",
      "Epoch [1/2], Step [400/2400], Loss: 0.7058\n",
      "Epoch [1/2], Step [500/2400], Loss: 0.6589\n",
      "Epoch [1/2], Step [600/2400], Loss: 0.8052\n",
      "Epoch [1/2], Step [700/2400], Loss: 0.8347\n",
      "Epoch [1/2], Step [800/2400], Loss: 0.8141\n",
      "Epoch [1/2], Step [900/2400], Loss: 0.6778\n",
      "Epoch [1/2], Step [1000/2400], Loss: 0.6705\n",
      "Epoch [1/2], Step [1100/2400], Loss: 0.7621\n",
      "Epoch [1/2], Step [1200/2400], Loss: 0.8300\n",
      "Epoch [1/2], Step [1300/2400], Loss: 0.6492\n",
      "Epoch [1/2], Step [1400/2400], Loss: 0.6272\n",
      "Epoch [1/2], Step [1500/2400], Loss: 0.6220\n",
      "Epoch [1/2], Step [1600/2400], Loss: 0.6599\n",
      "Epoch [1/2], Step [1700/2400], Loss: 0.6199\n",
      "Epoch [1/2], Step [1800/2400], Loss: 0.6768\n",
      "Epoch [1/2], Step [1900/2400], Loss: 0.5765\n",
      "Epoch [1/2], Step [2000/2400], Loss: 0.7814\n",
      "Epoch [1/2], Step [2100/2400], Loss: 0.7125\n",
      "Epoch [1/2], Step [2200/2400], Loss: 0.7287\n",
      "Epoch [1/2], Step [2300/2400], Loss: 0.6159\n",
      "Epoch [1/2], Step [2400/2400], Loss: 0.7132\n",
      "Epoch [2/2], Step [100/2400], Loss: 0.5943\n",
      "Epoch [2/2], Step [200/2400], Loss: 0.8227\n",
      "Epoch [2/2], Step [300/2400], Loss: 0.6174\n",
      "Epoch [2/2], Step [400/2400], Loss: 0.7214\n",
      "Epoch [2/2], Step [500/2400], Loss: 0.5989\n",
      "Epoch [2/2], Step [600/2400], Loss: 0.7769\n",
      "Epoch [2/2], Step [700/2400], Loss: 0.7228\n",
      "Epoch [2/2], Step [800/2400], Loss: 0.6116\n",
      "Epoch [2/2], Step [900/2400], Loss: 0.6341\n",
      "Epoch [2/2], Step [1000/2400], Loss: 0.6189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3030429/1607745371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 反向传播和优化，注意梯度每次清零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bento/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bento/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs, batch_seq_len)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化，注意梯度每次清零\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd39684bf14d2ca810b716d1040a79cb607a6203677f215342fe701ca54ee1b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
