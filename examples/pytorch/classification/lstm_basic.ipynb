{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "\n",
    "sys.path.append('/home/chaipf/work')\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX  import SummaryWriter\n",
    "\n",
    "from zh_nlp_demo.utils.tokenization import FullTokenizer\n",
    "\n",
    "# https://bravey.github.io/2020-05-12-使用LSTM+Pytorch对电影评论进行情感分类.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        # 并行gpu\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # cpu/gpu结果一致\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        # 训练集变化不大时使训练加速\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量：  9600\n",
      "验证集样本数量：  1200\n",
      "测试集样本数量：  1200\n",
      "训练集样本示例：\n",
      "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "[train_examples, dev_examples, test_examples] = load_dataset('chnsenticorp', splits=('train', 'dev', 'test'))\n",
    "\n",
    "print('训练集样本数量： ', len(train_examples))\n",
    "print('验证集样本数量： ', len(dev_examples))\n",
    "print('测试集样本数量： ', len(test_examples))\n",
    "\n",
    "print('训练集样本示例：')\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer('../../data/dict/vocab.txt')\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.data, self.label = self.examples_to_ids(tokenizer, examples)\n",
    "\n",
    "    def examples_to_ids(self, tokenizer, examples, maxlen=200, num_classes=2):\n",
    "        input_ids, input_y = [], []\n",
    "        for example in examples:\n",
    "            token = tokenizer.tokenize(example['text'])\n",
    "            if len(token) < maxlen:\n",
    "                token = ['[PAD]'] * (maxlen - len(token)) + token\n",
    "            else:\n",
    "                token = token[:maxlen]\n",
    "            input_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "            input_y.append(example['label'])\n",
    "\n",
    "        return input_ids, input_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    input_data, input_label = [], []\n",
    "    for text, label in batch:\n",
    "        input_data.append(text)\n",
    "        input_label.append(label)\n",
    "    \n",
    "    input_label = torch.tensor(input_label)\n",
    "    input_data = torch.tensor(input_data)\n",
    "    return input_data, input_label\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(tokenizer, train_examples)\n",
    "# iter = DatasetIterater(train_dataset, config.batch_size, config.device)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "dev_dataset = MyDataset(tokenizer, dev_examples)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = MyDataset(tokenizer, test_examples)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.6084384322166443\n",
      "epoch: 0  loss: 0.5750779509544373\n",
      "epoch: 0  loss: 0.5789977312088013\n",
      "epoch: 0  loss: 0.862703263759613\n",
      "epoch: 0  loss: 0.4099316895008087\n",
      "epoch: 0  loss: 0.4728037714958191\n",
      "Accuracy: 0.7725\n",
      "-------------------------------\n",
      "epoch: 1  loss: 0.658536434173584\n",
      "epoch: 1  loss: 0.4510433077812195\n",
      "epoch: 1  loss: 0.4529229402542114\n",
      "epoch: 1  loss: 0.5922995805740356\n",
      "epoch: 1  loss: 0.503049373626709\n",
      "epoch: 1  loss: 0.4114025831222534\n",
      "Accuracy: 0.8317\n",
      "-------------------------------\n",
      "epoch: 2  loss: 0.540065348148346\n",
      "epoch: 2  loss: 0.5054954290390015\n",
      "epoch: 2  loss: 0.46210166811943054\n",
      "epoch: 2  loss: 0.4591887295246124\n",
      "epoch: 2  loss: 0.3746809959411621\n",
      "epoch: 2  loss: 0.5050589442253113\n",
      "Accuracy: 0.8583\n",
      "-------------------------------\n",
      "epoch: 3  loss: 0.4993102550506592\n",
      "epoch: 3  loss: 0.5483786463737488\n",
      "epoch: 3  loss: 0.37605202198028564\n",
      "epoch: 3  loss: 0.4025033414363861\n",
      "epoch: 3  loss: 0.378490686416626\n",
      "epoch: 3  loss: 0.34969961643218994\n",
      "Accuracy: 0.8683\n",
      "-------------------------------\n",
      "epoch: 4  loss: 0.3912219703197479\n",
      "epoch: 4  loss: 0.38135766983032227\n",
      "epoch: 4  loss: 0.344146192073822\n",
      "epoch: 4  loss: 0.38520193099975586\n",
      "epoch: 4  loss: 0.3136962950229645\n",
      "epoch: 4  loss: 0.3893454670906067\n",
      "Accuracy: 0.8708\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        text_emb = self.embedding(x)\n",
    "        output, _ = self.lstm(text_emb)\n",
    "\n",
    "        text_fea = self.fc(output[:, -1, :])\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.softmax(text_fea, dim=1)\n",
    "        return text_out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# 实例化一个Birectional RNN模型\n",
    "model = BiRNN(vocab_size, input_size, hidden_size, num_classes).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 学习率调整\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "#一个epoch的训练逻辑\n",
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for iter_index, data in enumerate(train_data_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # 初始为0，清除上个batch的梯度信息\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_index % 100 == 99:\n",
    "            print('epoch: {}  loss: {}'.format(epoch, loss.item()))\n",
    "        # scheduler.step()\n",
    "\n",
    "\n",
    "def validation(model):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for data in dev_data_loader:\n",
    "        with torch.no_grad():\n",
    "            # 正常传播\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        logits = outputs\n",
    "\n",
    "        # total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_data_loader)\n",
    "    print('Accuracy: %.4f' % (avg_val_accuracy))\n",
    "    # print('Average testing loss: %.4f' % (total_eval_loss / len(test_data_loader)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(model, epoch)\n",
    "    validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer('../../data/dict/vocab.txt')\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.data, self.label = self.examples_to_ids(tokenizer, examples)\n",
    "\n",
    "    def examples_to_ids(self, tokenizer, examples, maxlen=200, num_classes=2):\n",
    "        tokens = [tokenizer.tokenize(example['text']) for example in examples]\n",
    "        input_ids = [torch.tensor(tokenizer.convert_tokens_to_ids(tokens), dtype=torch.int64) for tokens in tokens]\n",
    "        input_y = [example['label'] for example in examples]\n",
    "        return input_ids, input_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    data_length = [len(sq[0]) for sq in batch]\n",
    "    input_data, input_label = [], []\n",
    "    for text, label in batch:\n",
    "        input_data.append(text)\n",
    "        input_label.append(label)\n",
    "    input_data = pad_sequence(input_data, batch_first=True, padding_value=0)\n",
    "    input_label = torch.tensor(input_label)\n",
    "    return input_data, input_label, data_length\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(tokenizer, train_examples)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "dev_dataset = MyDataset(tokenizer, dev_examples)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = MyDataset(tokenizer, test_examples)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7332198619842529\n",
      "epoch: 0  loss: 0.6996564269065857\n",
      "Accuracy: 0.4942\n",
      "-------------------------------\n",
      "epoch: 0  loss: 0.6862127184867859\n",
      "epoch: 0  loss: 0.6927137970924377\n",
      "Accuracy: 0.4950\n",
      "-------------------------------\n",
      "epoch: 0  loss: 0.6892074942588806\n",
      "epoch: 0  loss: 0.6802089214324951\n",
      "Accuracy: 0.5033\n",
      "-------------------------------\n",
      "epoch: 0  loss: 0.7204221487045288\n",
      "epoch: 0  loss: 0.6712704300880432\n",
      "Accuracy: 0.4983\n",
      "-------------------------------\n",
      "epoch: 0  loss: 0.6740809679031372\n",
      "epoch: 0  loss: 0.6814371347427368\n",
      "Accuracy: 0.4967\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True, bidirectional=True)\n",
    "        # 隐层包含向前层和向后层两层，所以隐层共有两倍的Hidden_size\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "\n",
    "        text_emb = self.embedding(x)\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[:, 0, :]\n",
    "        # out_reverse = output[:, 0, :]\n",
    "        # out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\n",
    "        text_fea = self.fc(out_forward)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.softmax(text_fea, dim=1)\n",
    "        return text_out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# 实例化一个Birectional RNN模型\n",
    "model = BiRNN(vocab_size, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# 学习率调整\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10,gamma=0.1)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "#一个epoch的训练逻辑\n",
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    for iter_index, data in enumerate(train_data_loader):\n",
    "        inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "        # 初始为0，清除上个batch的梯度信息\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, batch_seq_len)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_index % 300 == 299:\n",
    "            print('epoch: {}  loss: {}'.format(epoch, loss.item()))\n",
    "        # scheduler.step()\n",
    "\n",
    "\n",
    "def validation(model):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for data in dev_data_loader:\n",
    "        with torch.no_grad():\n",
    "            # 正常传播\n",
    "            inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "            outputs = model(inputs, batch_seq_len)\n",
    "\n",
    "        logits = outputs\n",
    "\n",
    "        # total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_data_loader)\n",
    "    print('Accuracy: %.4f' % (avg_val_accuracy))\n",
    "    # print('Average testing loss: %.4f' % (total_eval_loss / len(test_data_loader)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(model, 0)\n",
    "    validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "\n",
    "\n",
    "#混淆矩阵指标\n",
    "class ConfuseMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 标签的分类：0 pos 1 neg \n",
    "        self.confuse_mat = torch.zeros(2,2)\n",
    "        self.tp = self.confuse_mat[0,0]\n",
    "        self.fp = self.confuse_mat[0,1]\n",
    "        self.tn = self.confuse_mat[1,1]\n",
    "        self.fn = self.confuse_mat[1,0]\n",
    "        self.acc = 0\n",
    "        self.pre = 0\n",
    "        self.rec = 0\n",
    "        self.F1 = 0\n",
    "    def update(self, output, label):\n",
    "        pred = output.argmax(dim = 1)\n",
    "        for l, p in zip(label.view(-1),pred.view(-1)):\n",
    "            self.confuse_mat[p.long(), l.long()] += 1 # 对应的格子加1\n",
    "        self.tp = self.confuse_mat[0,0]\n",
    "        self.fp = self.confuse_mat[0,1]\n",
    "        self.tn = self.confuse_mat[1,1]\n",
    "        self.fn = self.confuse_mat[1,0]\n",
    "        self.acc = (self.tp+self.tn) / self.confuse_mat.sum()\n",
    "        self.pre = self.tp / (self.tp + self.fp)\n",
    "        self.rec = self.tp / (self.tp + self.fn)\n",
    "        self.F1 = 2 * self.pre*self.rec / (self.pre + self.rec)\n",
    "\n",
    "\n",
    "## topk的准确率计算\n",
    "def accuracy(output, label, topk=(1,)):\n",
    "    maxk = max(topk) \n",
    "    batch_size = label.size(0)\n",
    "    \n",
    "    # 获取前K的索引\n",
    "    _, pred = output.topk(maxk, 1, True, True) # 使用topk来获得前k个的索引\n",
    "    pred = pred.t() # 进行转置\n",
    "    # eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape\n",
    "    # expand_as 执行按行复制来扩展，要保证列相等\n",
    "    correct = pred.eq(label.view(1, -1).expand_as(pred)) # 与正确标签序列形成的矩阵相比，生成True/False矩阵\n",
    "    # print(correct)\n",
    "    \n",
    "    rtn = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0) # 前k行的数据 然后平整到1维度，来计算true的总个数\n",
    "        rtn.append(correct_k.mul_(100.0 / batch_size)) # mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 隐层包含向前层和向后层两层，所以隐层共有两倍的Hidden_size\n",
    "    \n",
    "    def forward(self, x, text_len):\n",
    "\n",
    "        text_emb = self.embedding(x)\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[:, -1, :]\n",
    "        # out_reverse = output[:, 0, :]\n",
    "        # out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\n",
    "        text_fea = self.fc(out_forward)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "        return text_out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# 实例化一个Birectional RNN模型\n",
    "model = BiRNN(vocab_size, input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 学习率调整\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10,gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个epoch的训练逻辑\n",
    "def train(epoch, train_loader, device, model, criterion, optimizer, scheduler, tensorboard_path):\n",
    "    model.train()\n",
    "    top1 = AvgrageMeter()\n",
    "    model = model.to(device)\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):  # 0是下标起始位置默认为0\n",
    "        inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "        # 初始为0，清除上个batch的梯度信息\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,batch_seq_len)\n",
    "\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = outputs.topk(1)\n",
    "\n",
    "        prec1, prec2= accuracy(outputs, labels, topk=(1,2))\n",
    "        n = inputs.size(0)\n",
    "        top1.update(prec1.item(), n)\n",
    "        train_loss += loss.item()\n",
    "        postfix = {'train_loss': '%.6f' % (train_loss / (i + 1)), 'train_acc': '%.6f' % top1.avg}\n",
    "        train_loader.set_postfix(log=postfix)\n",
    "\n",
    "        # ternsorboard 曲线绘制\n",
    "        if os.path.exists(tensorboard_path) == False: \n",
    "            os.mkdir(tensorboard_path)    \n",
    "        writer = SummaryWriter(tensorboard_path)\n",
    "        writer.add_scalar('Train/Loss', loss.item(), epoch)\n",
    "        writer.add_scalar('Train/Accuracy', top1.avg, epoch)\n",
    "        writer.flush()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def validate(epoch, validate_loader, device, model, criterion, tensorboard_path):\n",
    "    val_acc = 0.0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # 进行评测的时候网络不更新梯度\n",
    "        val_top1 = AvgrageMeter()\n",
    "        validate_loader = tqdm(validate_loader)\n",
    "        validate_loss = 0.0\n",
    "        for i, data in enumerate(validate_loader):\n",
    "            inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "\n",
    "            outputs = model(inputs, batch_seq_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            prec1, prec2 = accuracy(outputs, labels, topk=(1, 2))\n",
    "            n = inputs.size(0)\n",
    "            val_top1.update(prec1.item(), n)\n",
    "            validate_loss += loss.item()\n",
    "            postfix = {'validate_loss': '%.6f' % (validate_loss / (i + 1)), 'validate_acc': '%.6f' % val_top1.avg}\n",
    "            validate_loader.set_postfix(log=postfix)\n",
    "            \n",
    "            # ternsorboard 曲线绘制\n",
    "            if os.path.exists(tensorboard_path) == False: \n",
    "                os.mkdir(tensorboard_path)    \n",
    "            writer = SummaryWriter(tensorboard_path)\n",
    "            writer.add_scalar('Validate/Loss', loss.item(), epoch)\n",
    "            writer.add_scalar('Validate/Accuracy', val_top1.avg, epoch)\n",
    "            writer.flush()\n",
    "        val_acc = val_top1.avg\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "def test(validate_loader, device, model, criterion):\n",
    "    val_acc = 0.0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    confuse_meter = ConfuseMeter()\n",
    "    with torch.no_grad():\n",
    "        val_top1 = AvgrageMeter()\n",
    "        validate_loader = tqdm(validate_loader)\n",
    "        validate_loss = 0.0\n",
    "        for i, data in enumerate(validate_loader, 0):  # 0是下标起始位置默认为0\n",
    "            inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "            outputs, _ = model(inputs, batch_seq_len)\n",
    "\n",
    "            prec1, prec2 = accuracy(outputs, labels, topk=(1, 2))\n",
    "            n = inputs.size(0)\n",
    "            val_top1.update(prec1.item(), n)\n",
    "            confuse_meter.update(outputs, labels)\n",
    "            postfix = {\n",
    "                'test_acc': '%.6f' % val_top1.avg,\n",
    "                'confuse_acc': '%.6f' % confuse_meter.acc\n",
    "            }\n",
    "            validate_loader.set_postfix(log=postfix)\n",
    "        val_acc = val_top1.avg\n",
    "    return confuse_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc2f2985142445d95eca3b28be21a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3068a5ab681e4d81a6b4afd7a2aedf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058d783f2d0d437497d5c18e3aebb873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec391fc0f1843d1bfd4273a45fb88a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard_path = './log'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_data_loader = tqdm(train_data_loader)\n",
    "    train_data_loader.set_description('[%s%04d/%04d %s%f]' % ('Epoch:', epoch + 1, num_epochs, 'lr:', scheduler.get_lr()[0]))\n",
    "    \n",
    "    train(epoch, train_data_loader, device, model, criterion, optimizer,scheduler, tensorboard_path)\n",
    "    validate(epoch, dev_data_loader, device, model, criterion, tensorboard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B1.116.80.65/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# 反向传播和优化，注意梯度每次清零\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B1.116.80.65/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B1.116.80.65/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B1.116.80.65/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B1.116.80.65/home/chaipf/work/zh_nlp_demo/pytorch/classification/lstm_basic.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/zh_nlp/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/zh_nlp/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "total_step = len(train_data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        inputs, labels, batch_seq_len = data[0].to(device), data[1].to(device), data[2]\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs, batch_seq_len)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化，注意梯度每次清零\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zh_nlp",
   "language": "python",
   "name": "zh_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46f7d3244a40cfb406c613661afc3f3d0b4286ecfdd7878593e4f054618c9137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
